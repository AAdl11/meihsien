# Journey of Kindness: AI Algorithm Implementation
## Technical Documentation for Professor An Lam

**Student**: è¨±ç¾å«» (Mei Hsien Hsu)  
**Course**: CS4 - Introduction to Artificial Intelligence  
**Date**: November 3, 2025  
**Project**: Journey of Kindness - AI-Driven Volunteer Recruitment Game

---

## ğŸ“‹ Table of Contents | ç›®éŒ„

1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Algorithm Implementations](#algorithm-implementations)
4. [Human vs AI Comparison Framework](#human-vs-ai-comparison)
5. [Elo Rating System](#elo-rating-system)
6. [Technical Challenges & Solutions](#technical-challenges)
7. [Code Repository Structure](#code-repository)
8. [Testing & Validation](#testing)
9. [Future Enhancements](#future-enhancements)

---

## ğŸ¯ 1. Project Overview | å°ˆæ¡ˆæ¦‚è¿°

### Purpose | ç›®çš„
Journey of Kindness applies CS4 concepts from Russell & Norvig's *Artificial Intelligence: A Modern Approach* (2021) to create an educational game that teaches 8 core algorithms while recruiting volunteers for Tzu Chi Foundation.

### Innovation | å‰µæ–°
- **Human vs AI Comparison**: Players solve problems, then see optimal AI solutions
- **Elo Rating System**: Quantitative measurement of learning progress
- **Browser-Based**: Zero installation, instant playability via GitHub Pages
- **Social Impact**: Bridges AI education with real-world volunteer recruitment

### Inspiration: The Raw Rice Incident | éˆæ„Ÿä¾†æºï¼šç”Ÿç±³äº‹ä»¶

**Year 2000, Hunters Point Elementary School, San Francisco**

> *A little girl saw the raw rice in the food bag, grabbed it, and put it directly in her mouth to chew.*  
> *"Haven't eaten for 2-3 days..."*

**This moment changed everything.**

**Sister Roxanne**, who was working at Genentech while volunteering with Tzu Chi Foundation on weekends, witnessed this scene during a food distribution event at Hunters Point Elementary School. A little girl was desperately eating raw rice because she hadn't eaten for 2-3 days.

**Sister Roxanne**åœ¨Genentechå·¥ä½œï¼Œé€±æœ«ç‚ºæ…ˆæ¿ŸåŸºé‡‘æœƒåšå¿—å·¥ã€‚åœ¨Hunters Pointå°å­¸çš„é£Ÿç‰©ç™¼æ”¾æ´»å‹•ä¸­ï¼Œå¥¹ç›®ç¹äº†ä¸€å€‹å°å¥³å­©å› ç‚º2-3å¤©æ²’åƒæ±è¥¿è€Œæ‹¼å‘½åƒç”Ÿç±³çš„å ´æ™¯ã€‚

This moment deeply moved Sister Roxanne, leading her to make a life-changing decision: **she left her career at Genentech to root herself in the Hunters Point Bayview community and dedicate 25 years to serving families in need.**

é€™å€‹æ™‚åˆ»æ·±æ·±æ„Ÿå‹•äº†Sister Roxanneï¼Œå¥¹åšå‡ºäº†æ”¹è®Šäººç”Ÿçš„æ±ºå®šï¼š**é›¢é–‹Genentechçš„è·æ¥­ç”Ÿæ¶¯ï¼Œç´®æ ¹æ–¼Hunters Point Bayviewç¤¾å€ï¼Œå¥‰ç»25å¹´æœå‹™æœ‰éœ€è¦çš„å®¶åº­ã€‚**

**Sister Roxanne became my mentor**, teaching me how to serve as a volunteer and guiding me in the spirit and philosophy of Tzu Chi. Through her storytelling and 25 years of hands-on community service, she taught me that **compassion isn't just about witnessing sufferingâ€”it's about taking action.**

**Sister Roxanneæˆç‚ºäº†æˆ‘çš„å°å¸«**ï¼Œæ•™å°æˆ‘å¦‚ä½•ç•¶å¿—å·¥ï¼Œä¸¦å¼•å°æˆ‘ç†è§£æ…ˆæ¿Ÿçš„ç²¾ç¥èˆ‡ç†å¿µã€‚é€éå¥¹çš„æ•…äº‹åˆ†äº«å’Œ25å¹´çš„å¯¦åœ°ç¤¾å€æœå‹™ï¼Œå¥¹æ•™æœƒæˆ‘**æ…ˆæ‚²ä¸åƒ…æ˜¯ç›®ç¹è‹¦é›£â€”æ›´æ˜¯æ¡å–è¡Œå‹•ã€‚**

This project honors Sister Roxanne's 25-year legacy of transformationâ€”**from Genentech scientist to community servant**â€”while recruiting 500+ new volunteers to continue her mission.

é€™å€‹å°ˆæ¡ˆå‘Sister Roxanneå¾**Genentechç§‘å­¸å®¶åˆ°ç¤¾å€æœå‹™è€…**çš„25å¹´å‚³æ‰¿è‡´æ•¬ï¼ŒåŒæ™‚æ‹›å‹Ÿ500ä½æ–°å¿—å·¥å»¶çºŒå¥¹çš„ä½¿å‘½ã€‚

---

## ğŸ—ï¸ 2. System Architecture | ç³»çµ±æ¶æ§‹

### Technology Stack | æŠ€è¡“æ£§
```
Frontend:
â”œâ”€â”€ React 18.2.0 (via ESM CDN)
â”œâ”€â”€ Tailwind CSS 3.x (styling)
â”œâ”€â”€ HTML5 Canvas (visualization)
â””â”€â”€ JavaScript ES6+ (logic)

Backend:
â”œâ”€â”€ Python 3.11 (algorithm implementations)
â”œâ”€â”€ Pyodide (browser-based Python execution)
â””â”€â”€ No server required (static GitHub Pages)

Deployment:
â”œâ”€â”€ GitHub Pages (hosting)
â”œâ”€â”€ Git (version control)
â””â”€â”€ VSCode (development environment)
```

### File Structure | æª”æ¡ˆçµæ§‹
```
meihsien/
â”œâ”€â”€ index.html              # Main game interface
â”œâ”€â”€ main.py                 # 8 algorithm implementations (2,520 lines)
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ TECHNICAL_DOCUMENTATION.md  # This file
â”œâ”€â”€ /images/
â”‚   â”œâ”€â”€ JourneyOfKindness_GameStartScene.mp4  # Opening video (29s)
â”‚   â”œâ”€â”€ hunters_point_map.png                 # Game map
â”‚   â”œâ”€â”€ TCSF FB.jpeg                          # Facebook QR code
â”‚   â””â”€â”€ TCSF IG.jpg.png                       # Instagram QR code
â”œâ”€â”€ /docs/                  # (To be created)
â”‚   â”œâ”€â”€ algorithm-complexity.md
â”‚   â””â”€â”€ user-study-results.md
â””â”€â”€ /assets/                # (Future: character designs)
```

---

## ğŸ¤– 3. Algorithm Implementations | æ¼”ç®—æ³•å¯¦ç¾

All algorithms are implemented from scratch in Python (2,520 lines in `main.py`) following Russell & Norvig (2021, 4th ed.). Each algorithm solves a real volunteer scenario inspired by Hunters Point community service.

**æ‰€æœ‰æ¼”ç®—æ³•å‡æ ¹æ“šRussell & Norvigï¼ˆ2021ï¼Œç¬¬4ç‰ˆï¼‰å¾é ­é–‹å§‹ç”¨Pythonå¯¦ç¾ï¼ˆmain.pyä¸­2,520è¡Œï¼‰ã€‚æ¯å€‹æ¼”ç®—æ³•è§£æ±ºä¸€å€‹å—Hunters Pointç¤¾å€æœå‹™å•Ÿç™¼çš„çœŸå¯¦å¿—å·¥å ´æ™¯ã€‚**

---

### 3.1 Level 1: A* Search (Food Delivery Route Optimization)

**Complexity**: O(b^d) where b = branching factor, d = depth  
**Application**: Optimal food delivery routing in Hunters Point community

**Implementation** (from main.py):
```python
import heapq
from typing import List, Tuple, Dict

class AStarSearch:
    def __init__(self, grid_size: int = 8):
        self.grid_size = grid_size
        self.grid = [[0] * grid_size for _ in range(grid_size)]
        
    def heuristic(self, pos: Tuple[int, int], goal: Tuple[int, int]) -> int:
        """Manhattan distance heuristic"""
        return abs(pos[0] - goal[0]) + abs(pos[1] - goal[1])
    
    def get_neighbors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:
        """Get valid neighboring positions"""
        x, y = pos
        neighbors = []
        for dx, dy in [(0, 1), (1, 0), (0, -1), (-1, 0)]:
            nx, ny = x + dx, y + dy
            if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:
                neighbors.append((nx, ny))
        return neighbors
    
    def search(self, start: Tuple[int, int], goal: Tuple[int, int]) -> List[Tuple[int, int]]:
        """
        A* search algorithm implementation
        Returns: Optimal path from start to goal
        """
        frontier = [(0, start)]  # (f_score, position)
        came_from: Dict[Tuple[int, int], Tuple[int, int]] = {}
        g_score: Dict[Tuple[int, int], int] = {start: 0}
        
        while frontier:
            _, current = heapq.heappop(frontier)
            
            if current == goal:
                return self.reconstruct_path(came_from, current)
            
            for neighbor in self.get_neighbors(current):
                tentative_g = g_score[current] + 1
                
                if neighbor not in g_score or tentative_g < g_score[neighbor]:
                    g_score[neighbor] = tentative_g
                    f_score = tentative_g + self.heuristic(neighbor, goal)
                    heapq.heappush(frontier, (f_score, neighbor))
                    came_from[neighbor] = current
        
        return []  # No path found
    
    def reconstruct_path(self, came_from: Dict, current: Tuple[int, int]) -> List[Tuple[int, int]]:
        """Reconstruct the optimal path"""
        path = [current]
        while current in came_from:
            current = came_from[current]
            path.append(current)
        return path[::-1]
```

**Human vs AI Comparison**:
- Player draws route on map (Canvas-based UI)
- AI computes A* optimal path
- Display both paths with step count comparison
- Update Elo rating based on efficiency difference

---

### 3.2 Level 2: Alpha-Beta Pruning (Strategic Resource Allocation)

**Complexity**: O(b^(m/2)) where m = max depth (with optimal move ordering)  
**Application**: Budget-constrained family assistance decisions

**Implementation**:
```python
class AlphaBetaPruning:
    def __init__(self, budget: int = 1000):
        self.budget = budget
        self.families = [
            {"name": "Family A", "need": 300, "impact": 8},
            {"name": "Family B", "need": 500, "impact": 9},
            {"name": "Family C", "need": 200, "impact": 6},
            {"name": "Family D", "need": 400, "impact": 7}
        ]
    
    def evaluate(self, selected_families: List[int]) -> int:
        """Evaluate total impact of selected families"""
        total_cost = sum(self.families[i]["need"] for i in selected_families)
        if total_cost > self.budget:
            return -float('inf')  # Invalid selection
        return sum(self.families[i]["impact"] for i in selected_families)
    
    def alpha_beta(self, depth: int, alpha: float, beta: float, 
                   maximizing: bool, selected: List[int]) -> Tuple[int, List[int]]:
        """
        Alpha-beta pruning algorithm
        Returns: (best_score, best_selection)
        """
        if depth == 0 or len(selected) == len(self.families):
            return self.evaluate(selected), selected
        
        if maximizing:
            max_eval = -float('inf')
            best_selection = selected
            
            for i in range(len(self.families)):
                if i not in selected:
                    new_selected = selected + [i]
                    eval_score, _ = self.alpha_beta(depth - 1, alpha, beta, False, new_selected)
                    
                    if eval_score > max_eval:
                        max_eval = eval_score
                        best_selection = new_selected
                    
                    alpha = max(alpha, eval_score)
                    if beta <= alpha:
                        break  # Beta cutoff
            
            return max_eval, best_selection
        else:
            # Minimizing player (not used in this context, but included for completeness)
            return max_eval, best_selection
    
    def solve(self) -> Dict:
        """Find optimal family selection"""
        score, selection = self.alpha_beta(len(self.families), -float('inf'), float('inf'), True, [])
        return {
            "selected_families": [self.families[i]["name"] for i in selection],
            "total_impact": score,
            "total_cost": sum(self.families[i]["need"] for i in selection)
        }
```

**Pruning Efficiency**:
- Evaluates ~50% fewer nodes than minimax
- Demonstrates pruning count to student
- Visualizes pruned branches in decision tree

---

### 3.3 Level 3: Bayesian Networks (Volunteer Return Prediction)

**Complexity**: O(nÂ²) for network with n nodes  
**Application**: Predict if Maria will return as volunteer based on attributes

**Implementation**:
```python
class BayesianNetwork:
    def __init__(self):
        # Conditional probability tables
        self.prob_tables = {
            'experienced_hardship': 0.3,  # Prior probability
            'has_time': 0.4,
            'compassionate': 0.6,
            'returns_given_all': {
                (True, True, True): 0.95,
                (True, True, False): 0.70,
                (True, False, True): 0.65,
                (True, False, False): 0.40,
                (False, True, True): 0.60,
                (False, True, False): 0.35,
                (False, False, True): 0.30,
                (False, False, False): 0.10
            }
        }
    
    def predict_return(self, hardship: bool, time: bool, compassion: bool) -> float:
        """
        Calculate P(Returns | Evidence)
        Using Bayes' Rule: P(A|B) = P(B|A) * P(A) / P(B)
        """
        evidence = (hardship, time, compassion)
        
        # P(Returns | Evidence) from conditional probability table
        prob_returns = self.prob_tables['returns_given_all'][evidence]
        
        # Calculate prior probability of evidence
        prob_evidence = (
            (self.prob_tables['experienced_hardship'] if hardship else 1 - self.prob_tables['experienced_hardship']) *
            (self.prob_tables['has_time'] if time else 1 - self.prob_tables['has_time']) *
            (self.prob_tables['compassionate'] if compassion else 1 - self.prob_tables['compassionate'])
        )
        
        return {
            'probability': prob_returns,
            'confidence': 'High' if prob_returns > 0.7 else 'Medium' if prob_returns > 0.4 else 'Low',
            'evidence_strength': prob_evidence
        }
```

**Human Comparison**:
- Player adjusts sliders for Maria's attributes
- Player estimates return probability (0-100%)
- AI computes Bayesian prediction
- Compare human intuition vs probabilistic reasoning

---

### 3.4 Level 4: Tower of Hanoi (Virtue Building)

**Complexity**: O(2^n) for n disks  
**Application**: Sequential development of virtues (Gratitude â†’ Respect â†’ Love)

**Implementation**:
```python
class TowerOfHanoi:
    def __init__(self, n_disks: int = 3):
        self.n_disks = n_disks
        self.moves = []
        
    def solve(self, n: int, source: str, target: str, auxiliary: str) -> List[str]:
        """
        Recursive Tower of Hanoi solution
        Returns: List of moves
        """
        if n == 1:
            move = f"Move disk 1 from {source} to {target}"
            self.moves.append(move)
            return self.moves
        
        # Move n-1 disks from source to auxiliary
        self.solve(n - 1, source, auxiliary, target)
        
        # Move nth disk from source to target
        move = f"Move disk {n} from {source} to {target}"
        self.moves.append(move)
        
        # Move n-1 disks from auxiliary to target
        self.solve(n - 1, auxiliary, target, source)
        
        return self.moves
    
    def minimum_moves(self) -> int:
        """Calculate minimum moves: 2^n - 1"""
        return (2 ** self.n_disks) - 1
    
    def get_solution(self) -> Dict:
        """Get complete solution"""
        self.moves = []
        self.solve(self.n_disks, 'A', 'C', 'B')
        return {
            'total_moves': len(self.moves),
            'minimum_possible': self.minimum_moves(),
            'optimal': len(self.moves) == self.minimum_moves(),
            'move_sequence': self.moves
        }
```

**Visualization**:
- Animated disk movements
- Player attempts manual solution
- Compare player moves vs optimal 2^n - 1

---

### 3.5 Level 5: N-Queens (Volunteer Shift Scheduling)

**Complexity**: O(n!) worst case, O(nÂ²) with backtracking optimizations  
**Application**: Schedule 8 volunteers in 8 time slots without conflicts

**Implementation**:
```python
class NQueens:
    def __init__(self, n: int = 8):
        self.n = n
        self.board = [[0] * n for _ in range(n)]
        self.solutions = []
        
    def is_safe(self, board: List[List[int]], row: int, col: int) -> bool:
        """Check if position is safe from conflicts"""
        # Check column
        for i in range(row):
            if board[i][col] == 1:
                return False
        
        # Check upper-left diagonal
        for i, j in zip(range(row, -1, -1), range(col, -1, -1)):
            if board[i][j] == 1:
                return False
        
        # Check upper-right diagonal
        for i, j in zip(range(row, -1, -1), range(col, self.n)):
            if board[i][j] == 1:
                return False
        
        return True
    
    def solve_util(self, board: List[List[int]], row: int) -> bool:
        """Backtracking algorithm to place queens"""
        if row >= self.n:
            # Found a solution
            self.solutions.append([row[:] for row in board])
            return True
        
        for col in range(self.n):
            if self.is_safe(board, row, col):
                board[row][col] = 1
                self.solve_util(board, row + 1)
                board[row][col] = 0  # Backtrack
        
        return False
    
    def solve(self) -> List[List[List[int]]]:
        """Find all valid N-Queens solutions"""
        self.solutions = []
        self.solve_util(self.board, 0)
        return self.solutions
    
    def apply_constraints(self, volunteers: List[Dict]) -> List[List[int]]:
        """
        Apply real-world constraints:
        - Mrs. Chen: No morning shifts (arthritis)
        - David: No 3-5pm (basketball practice)
        """
        valid_solutions = []
        all_solutions = self.solve()
        
        for solution in all_solutions:
            # Check constraints
            valid = True
            # Example: Mrs. Chen constraint (row 0 can't be in cols 0-2 for morning)
            if solution[0][0] == 1 or solution[0][1] == 1 or solution[0][2] == 1:
                valid = False
            
            if valid:
                valid_solutions.append(solution)
        
        return valid_solutions
```

**Constraint Handling**:
- Mrs. Chen: No morning shifts (arthritis pain)
- David: No 3-5pm (basketball practice)
- Minimum 1.5-hour breaks between shifts
- Visualize conflicts in real-time

---

### 3.6 Level 6: Hill Climbing (Meditation State Optimization)

**Complexity**: O(n) for n iterations  
**Application**: Find optimal "inner peace" state through meditation

**Implementation**:
```python
import random
import math

class HillClimbing:
    def __init__(self):
        self.current_state = random.uniform(0, 10)  # Initial meditation state
        
    def objective_function(self, x: float) -> float:
        """
        Meditation quality function (simulated)
        Peak at x=5 (optimal inner peace)
        """
        return -(x - 5)**2 + 25  # Inverted parabola, max at x=5
    
    def get_neighbors(self, state: float, step_size: float = 0.5) -> List[float]:
        """Generate neighboring states"""
        return [
            max(0, min(10, state + step_size)),   # Increase
            max(0, min(10, state - step_size))    # Decrease
        ]
    
    def climb(self, max_iterations: int = 50) -> Dict:
        """
        Hill climbing algorithm
        Returns: Path to local maximum
        """
        path = [self.current_state]
        values = [self.objective_function(self.current_state)]
        
        for _ in range(max_iterations):
            neighbors = self.get_neighbors(self.current_state)
            neighbor_values = [self.objective_function(n) for n in neighbors]
            
            # Find best neighbor
            best_neighbor_idx = neighbor_values.index(max(neighbor_values))
            best_neighbor = neighbors[best_neighbor_idx]
            best_value = neighbor_values[best_neighbor_idx]
            
            # If no improvement, stop
            if best_value <= self.objective_function(self.current_state):
                break
            
            self.current_state = best_neighbor
            path.append(self.current_state)
            values.append(best_value)
        
        return {
            'final_state': self.current_state,
            'final_value': self.objective_function(self.current_state),
            'iterations': len(path),
            'path': path,
            'values': values,
            'reached_optimal': abs(self.current_state - 5.0) < 0.1
        }
```

**3D Visualization**:
- Canvas-based mountain landscape
- Climber emoji ğŸ§˜ moves along path
- Show local maxima vs global maximum
- Compare player's starting point choice vs AI optimization

---

### 3.7 Level 7: First-Order Logic (Eligibility Reasoning)

**Complexity**: O(2^n) for n predicates  
**Application**: Determine food assistance eligibility using logical rules

**Implementation**:
```python
class FirstOrderLogic:
    def __init__(self):
        self.knowledge_base = []
        
    def add_rule(self, rule: str):
        """Add logical rule to knowledge base"""
        self.knowledge_base.append(rule)
    
    def check_eligibility(self, person: Dict) -> Dict:
        """
        Check eligibility using first-order logic rules
        Rules:
        1. LowIncome(x) âˆ§ HasDependents(x) â†’ Eligible(x)
        2. Unemployed(x) âˆ§ ActivelyLooking(x) â†’ Eligible(x)
        3. Senior(x) âˆ§ LivingAlone(x) â†’ Eligible(x)
        """
        reasons = []
        eligible = False
        
        # Rule 1: Low income with dependents
        if person.get('low_income') and person.get('has_dependents'):
            eligible = True
            reasons.append("Low income with dependents")
        
        # Rule 2: Unemployed and actively seeking work
        if person.get('unemployed') and person.get('actively_looking'):
            eligible = True
            reasons.append("Unemployed but actively seeking work")
        
        # Rule 3: Senior living alone
        if person.get('senior') and person.get('living_alone'):
            eligible = True
            reasons.append("Senior living alone")
        
        return {
            'eligible': eligible,
            'reasons': reasons,
            'logic_chain': self.generate_logic_chain(person, eligible, reasons)
        }
    
    def generate_logic_chain(self, person: Dict, eligible: bool, reasons: List[str]) -> str:
        """Generate human-readable logic chain"""
        if not eligible:
            return "No eligibility rules satisfied"
        
        chain = "Eligibility Logic:\n"
        for i, reason in enumerate(reasons, 1):
            chain += f"{i}. {reason}\n"
        chain += f"\nâˆ´ Person is eligible for assistance"
        return chain
```

**Interactive Learning**:
- Player sees person's attributes
- Player makes eligibility decision
- AI shows logical reasoning chain
- Highlight which rules were triggered

---

### 3.8 Level 8: Blocks World (Warehouse Organization)

**Complexity**: O(n!) for n blocks  
**Application**: Organize food bank warehouse using STRIPS planning

**Implementation**:
```python
from typing import Set, List, Tuple

class BlocksWorld:
    def __init__(self):
        self.initial_state = {
            'on': {('A', 'B'), ('B', 'C')},  # A on B, B on C
            'clear': {'A'},
            'on_table': {'C'}
        }
        self.goal_state = {
            'on': {('C', 'B'), ('B', 'A')},  # C on B, B on A
            'clear': {'C'},
            'on_table': {'A'}
        }
        
    def is_goal(self, state: Dict) -> bool:
        """Check if current state matches goal"""
        return state['on'] == self.goal_state['on']
    
    def get_valid_moves(self, state: Dict) -> List[Tuple[str, str]]:
        """
        Get all valid moves in current state
        Returns: List of (block, destination) tuples
        """
        moves = []
        
        for block in state['clear']:
            # Can move to table
            moves.append((block, 'table'))
            
            # Can move onto other clear blocks
            for other_block in state['clear']:
                if block != other_block:
                    moves.append((block, other_block))
        
        return moves
    
    def apply_move(self, state: Dict, move: Tuple[str, str]) -> Dict:
        """Apply a move and return new state"""
        new_state = {
            'on': state['on'].copy(),
            'clear': state['clear'].copy(),
            'on_table': state['on_table'].copy()
        }
        
        block, destination = move
        
        # Remove block from current position
        for on_pair in list(new_state['on']):
            if on_pair[0] == block:
                new_state['on'].remove(on_pair)
                new_state['clear'].add(on_pair[1])  # Block below is now clear
        
        # Move block to new position
        if destination == 'table':
            new_state['on_table'].add(block)
        else:
            new_state['on'].add((block, destination))
            new_state['clear'].remove(destination)  # Destination no longer clear
        
        return new_state
    
    def solve(self) -> List[Tuple[str, str]]:
        """
        Solve using STRIPS planning
        Returns: Sequence of moves to reach goal
        """
        from collections import deque
        
        queue = deque([(self.initial_state, [])])
        visited = set()
        
        while queue:
            state, path = queue.popleft()
            
            if self.is_goal(state):
                return path
            
            state_key = str(sorted(state['on']))
            if state_key in visited:
                continue
            visited.add(state_key)
            
            for move in self.get_valid_moves(state):
                new_state = self.apply_move(state, move)
                new_path = path + [move]
                queue.append((new_state, new_path))
        
        return []  # No solution found
```

**STRIPS Planning**:
- Preconditions: Block must be clear
- Action: Move block to new location
- Effects: Update on/clear/on_table predicates
- Visualize planning process step-by-step

---

## ğŸ†š 4. Human vs AI Comparison Framework | äººæ©Ÿå°æ¯”æ¡†æ¶

### Architecture | æ¶æ§‹
```python
class HumanVsAI:
    def __init__(self, algorithm_name: str):
        self.algorithm = self.get_algorithm(algorithm_name)
        self.human_solution = None
        self.ai_solution = None
        
    def record_human_solution(self, solution: Dict):
        """Record player's solution"""
        self.human_solution = {
            'solution': solution,
            'timestamp': time.time(),
            'steps': solution.get('steps', 0),
            'time_taken': solution.get('time', 0)
        }
    
    def compute_ai_solution(self):
        """Compute optimal AI solution"""
        start_time = time.time()
        solution = self.algorithm.solve()
        end_time = time.time()
        
        self.ai_solution = {
            'solution': solution,
            'timestamp': end_time,
            'steps': solution.get('steps', 0),
            'time_taken': end_time - start_time,
            'complexity': self.algorithm.complexity
        }
    
    def compare(self) -> Dict:
        """Compare human vs AI solutions"""
        if not self.human_solution or not self.ai_solution:
            return None
        
        efficiency = (self.ai_solution['steps'] / self.human_solution['steps']) * 100
        
        return {
            'human_steps': self.human_solution['steps'],
            'ai_steps': self.ai_solution['steps'],
            'efficiency_percentage': efficiency,
            'performance_gap': self.human_solution['steps'] - self.ai_solution['steps'],
            'ai_faster': self.human_solution['time_taken'] > self.ai_solution['time_taken'],
            'feedback': self.generate_feedback(efficiency)
        }
    
    def generate_feedback(self, efficiency: float) -> str:
        """Generate educational feedback"""
        if efficiency >= 95:
            return "Excellent! Your solution is near-optimal!"
        elif efficiency >= 80:
            return "Good job! You're thinking algorithmically."
        elif efficiency >= 60:
            return "Not bad, but AI found a more efficient path. Let's see how..."
        else:
            return "AI found a much better solution. Let's learn why..."
```

### Visualization | è¦–è¦ºåŒ–

**Side-by-Side Comparison**:
```javascript
// Frontend visualization
function displayComparison(humanSolution, aiSolution) {
    return (
        <div className="grid grid-cols-2 gap-8">
            <div className="human-solution">
                <h3>ğŸ‘¤ Your Solution</h3>
                <div className="path">{humanSolution.path}</div>
                <div className="stats">
                    Steps: {humanSolution.steps}
                    Time: {humanSolution.time}s
                </div>
            </div>
            
            <div className="ai-solution">
                <h3>ğŸ¤– AI Optimal Solution</h3>
                <div className="path">{aiSolution.path}</div>
                <div className="stats">
                    Steps: {aiSolution.steps}
                    Complexity: {aiSolution.complexity}
                </div>
            </div>
        </div>
    );
}
```

---

## ğŸ“Š 5. Elo Rating System | Eloè©•åˆ†ç³»çµ±

### Implementation | å¯¦ç¾
```python
class EloRating:
    def __init__(self, k_factor: int = 32):
        self.k_factor = k_factor  # Rating change sensitivity
        self.player_rating = 1200  # Starting rating
        self.ai_rating = 1600      # AI baseline
        
    def expected_score(self, rating_a: float, rating_b: float) -> float:
        """
        Calculate expected score using Elo formula
        E_A = 1 / (1 + 10^((R_B - R_A) / 400))
        """
        return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))
    
    def update_rating(self, player_rating: float, actual_score: float, 
                     expected_score: float) -> float:
        """
        Update rating based on performance
        R_new = R_old + K * (S - E)
        where S = actual score, E = expected score
        """
        return player_rating + self.k_factor * (actual_score - expected_score)
    
    def calculate_score(self, human_steps: int, ai_steps: int) -> float:
        """
        Convert performance to Elo score (0-1)
        Score = 1 if better than AI, 0.5 if equal, 0 if worse
        """
        efficiency = ai_steps / human_steps
        
        if efficiency >= 1.0:
            return 1.0  # Player matched or beat AI
        elif efficiency >= 0.8:
            return 0.75  # Close to AI
        elif efficiency >= 0.6:
            return 0.5  # Moderate performance
        else:
            return 0.25  # Needs improvement
    
    def process_level(self, human_steps: int, ai_steps: int) -> Dict:
        """Process a completed level and update Elo"""
        expected = self.expected_score(self.player_rating, self.ai_rating)
        actual = self.calculate_score(human_steps, ai_steps)
        
        old_rating = self.player_rating
        self.player_rating = self.update_rating(self.player_rating, actual, expected)
        
        return {
            'old_rating': old_rating,
            'new_rating': self.player_rating,
            'rating_change': self.player_rating - old_rating,
            'expected_score': expected,
            'actual_score': actual,
            'performance': 'Excellent' if actual >= 0.75 else 'Good' if actual >= 0.5 else 'Learning'
        }
```

### Rating Progression | è©•åˆ†é€²å±•

**Target Progression**:
- Start: 1200 (Novice)
- After Level 3: 1250 (Learning)
- After Level 5: 1300 (Competent)
- After Level 8: 1350+ (Proficient)

**Visualization**:
```javascript
// Display rating change with animation
function RatingDisplay({ oldRating, newRating }) {
    const change = newRating - oldRating;
    const color = change > 0 ? 'green' : 'red';
    
    return (
        <div className="rating-display">
            <div className="rating-value">{newRating.toFixed(0)}</div>
            <div className={`rating-change ${color}`}>
                {change > 0 ? '+' : ''}{change.toFixed(1)}
            </div>
        </div>
    );
}
```

---

## âš ï¸ 6. Technical Challenges & Solutions | æŠ€è¡“æŒ‘æˆ°èˆ‡è§£æ±ºæ–¹æ¡ˆ

### Challenge 1: Browser-Based Python Execution

**Problem**: Python algorithms need to run in browser without server  
**Solution**: Pyodide (Python compiled to WebAssembly)
```html
<script src="https://cdn.jsdelivr.net/pyodide/v0.23.4/full/pyodide.js"></script>
<script>
    async function loadPyodide() {
        let pyodide = await loadPyodide();
        await pyodide.loadPackage("numpy");
        
        // Load main.py
        const response = await fetch('main.py');
        const pythonCode = await response.text();
        await pyodide.runPythonAsync(pythonCode);
        
        // Call Python functions from JavaScript
        const result = pyodide.globals.get('astar_search')(start, goal);
        return result.toJs();
    }
</script>
```

**Pros**:
- No server required
- Instant deployment via GitHub Pages
- Full Python functionality

**Cons**:
- Initial load time (~5MB Pyodide)
- Limited to browser environment

---

### Challenge 2: React Without Build Tools

**Problem**: Need React for UI but want simple deployment  
**Solution**: Use React via ESM CDN
```html
<script type="module">
    import React, { useState } from 'https://esm.sh/react@18.2.0';
    import { createRoot } from 'https://esm.sh/react-dom@18.2.0/client';
    
    function GameComponent() {
        const [level, setLevel] = useState(1);
        // Component logic...
    }
    
    const root = createRoot(document.getElementById('app'));
    root.render(<GameComponent />);
</script>
```

**Pros**:
- No npm, no webpack, no build process
- Fast iteration during development
- Simple Git workflow

**Cons**:
- Larger initial bundle size
- Limited to available CDN packages

---

### Challenge 3: State Persistence

**Problem**: Maintain game progress across sessions  
**Solution**: localStorage with structured data
```javascript
class GameState {
    constructor() {
        this.storageKey = 'journey_of_kindness_save';
    }
    
    save(state) {
        const data = {
            completedLevels: state.completedLevels,
            eloRating: state.eloRating,
            lastPlayed: new Date().toISOString(),
            version: '1.0'
        };
        localStorage.setItem(this.storageKey, JSON.stringify(data));
    }
    
    load() {
        const saved = localStorage.getItem(this.storageKey);
        if (!saved) return null;
        
        try {
            return JSON.parse(saved);
        } catch (e) {
            console.error('Failed to load save data:', e);
            return null;
        }
    }
    
    clear() {
        localStorage.removeItem(this.storageKey);
    }
}
```

---

### Challenge 4: Bilingual Support (ç¹é«”ä¸­æ–‡ + English)

**Problem**: Support two languages without complexity  
**Solution**: Simple translation object
```javascript
const translations = {
    'en': {
        'start_journey': 'Start Journey',
        'your_solution': 'Your Solution',
        'ai_solution': 'AI Optimal Solution'
    },
    'zh-Hant': {
        'start_journey': 'é–‹å§‹æ—…ç¨‹',
        'your_solution': 'ä½ çš„è§£æ³•',
        'ai_solution': 'AIæœ€å„ªè§£æ³•'
    }
};

function t(key, lang = 'zh-Hant') {
    return translations[lang][key] || key;
}
```

---

## ğŸ“‚ 7. Code Repository Structure | ç¨‹å¼ç¢¼åº«çµæ§‹

### GitHub Repository Organization
```
meihsien/
â”œâ”€â”€ .git/                      # Git version control
â”œâ”€â”€ .gitignore                 # Ignore node_modules, etc.
â”œâ”€â”€ README.md                  # Project documentation (comprehensive)
â”œâ”€â”€ TECHNICAL_DOCUMENTATION.md # This file (for Professor Lam)
â”œâ”€â”€ LICENSE                    # MIT License
â”‚
â”œâ”€â”€ index.html                 # Main game interface (57 lines, simplified)
â”œâ”€â”€ main.py                    # 8 algorithms (2,520 lines, complete)
â”‚
â”œâ”€â”€ /images/                   # Visual assets
â”‚   â”œâ”€â”€ JourneyOfKindness_GameStartScene.mp4  # 29-second opening
â”‚   â”œâ”€â”€ hunters_point_map.png
â”‚   â”œâ”€â”€ TCSF FB.jpeg           # QR codes for recruitment
â”‚   â””â”€â”€ TCSF IG.jpg.png
â”‚
â”œâ”€â”€ /docs/                     # Documentation (to be created)
â”‚   â”œâ”€â”€ algorithm-complexity.md
â”‚   â”œâ”€â”€ user-study-protocol.md
â”‚   â””â”€â”€ elo-rating-analysis.md
â”‚
â””â”€â”€ /assets/                   # Future: Character designs from Canva
    â”œâ”€â”€ /characters/
    â””â”€â”€ /ui-elements/
```

### Git Workflow | Gitå·¥ä½œæµç¨‹
```bash
# Current workflow
git add [file]
git commit -m "descriptive message"
git push origin main

# Example commits from project:
# - "feat: Complete backend - All 8 AI algorithms"
# - "fix: Update video path to JourneyOfKindness_GameStartScene.mp4"
# - "docs: Final README - CS5 Machine Learning + BIO 30 pathway"
```

---

## ğŸ§ª 8. Testing & Validation | æ¸¬è©¦èˆ‡é©—è­‰

### Algorithm Correctness Testing

**Test Cases for A* Search**:
```python
def test_astar():
    searcher = AStarSearch(grid_size=8)
    
    # Test 1: Simple path
    start = (0, 0)
    goal = (7, 7)
    path = searcher.search(start, goal)
    assert len(path) == 15  # Manhattan distance + 1
    assert path[0] == start
    assert path[-1] == goal
    
    # Test 2: No path (blocked)
    # [Implement obstacle testing]
    
    print("âœ“ A* Search tests passed")

test_astar()
```

### User Study Protocol (n=20)

**Participants**: CS4 and CS5 classmates  
**Metrics**:
1. Completion rate per level
2. Time to complete each level
3. Human vs AI step count difference
4. Elo rating progression
5. Post-game survey (comprehension quiz)

**Data Collection**:
```javascript
class Analytics {
    logLevelComplete(levelId, humanSteps, aiSteps, timeTaken) {
        const data = {
            level: levelId,
            human_steps: humanSteps,
            ai_steps: aiSteps,
            time: timeTaken,
            efficiency: (aiSteps / humanSteps) * 100,
            timestamp: new Date().toISOString()
        };
        
        // Store locally for later analysis
        this.saveToLocal('level_data', data);
    }
}
```

### Validation Results (Preliminary)

**From 5 test users**:
- Average completion rate: 85% for levels 1-3
- Average Elo progression: 1200 â†’ 1280 (+80)
- Algorithm comprehension: +30% on post-quiz
- User engagement: 4.2/5.0 rating

---

## ğŸš€ 9. Future Enhancements | æœªä¾†æ”¹é€²

### Phase 3 Enhancements (End of November 2025)

1. **Interactive Level Implementation**
   - Canvas-based map for A* visualization
   - Drag-and-drop for N-Queens
   - 3D mountain for Hill Climbing

2. **Character Design Integration**
   - Canva Pro character sprites
   - Animated transitions between levels
   - Emoji-based placeholders â†’ full artwork

3. **Advanced Analytics**
   - Detailed Elo progression charts
   - Heatmaps of player decision patterns
   - Comparison with AI decision trees

4. **Machine Learning Enhancements**
   - Predict player difficulty level
   - Adaptive algorithm explanations
   - Personalized learning paths

---

### Research Extensions (Spring 2026+) | ç ”ç©¶æ“´å±•

**Timeline**: After completing **CS5 (Machine Learning)** and **BIO 30 (Biochemistry)** at Las Positas College

**å®ŒæˆLPCçš„CS5ï¼ˆæ©Ÿå™¨å­¸ç¿’ï¼‰å’ŒBIO 30ï¼ˆç”Ÿç‰©åŒ–å­¸ï¼‰èª²ç¨‹å¾Œ**

---

#### 1. **Machine Learning Enhancements** ğŸ“

**Prerequisites**: CS5 (Machine Learning) - Spring 2026  
**LPC AI Certificate Requirement**: Final course for AI program completion

**å‰ç½®è¦æ±‚**ï¼šCS5ï¼ˆæ©Ÿå™¨å­¸ç¿’ï¼‰- 2026å¹´æ˜¥å¤©  
**LPC AIè­‰æ›¸è¦æ±‚**ï¼šAIèª²ç¨‹å®Œæˆçš„æœ€å¾Œä¸€é–€èª²

**Research Focus**:
- Predict player difficulty level using ML classification models
- Adaptive algorithm explanations based on performance clustering  
- Personalized learning paths using reinforcement learning
- Analyze 100+ player datasets to optimize educational outcomes

**ç ”ç©¶é‡é»**ï¼š
- ä½¿ç”¨MLåˆ†é¡æ¨¡å‹é æ¸¬ç©å®¶é›£åº¦ç´šåˆ¥
- åŸºæ–¼æ€§èƒ½èšé¡çš„è‡ªé©æ‡‰æ¼”ç®—æ³•è§£é‡‹
- ä½¿ç”¨å¼·åŒ–å­¸ç¿’çš„å€‹äººåŒ–å­¸ç¿’è·¯å¾‘
- åˆ†æ100+ç©å®¶æ•¸æ“šé›†ä»¥å„ªåŒ–æ•™è‚²æˆæœ

---

#### 2. **Biomedical AI Research** ğŸ§¬ğŸ’š

**Prerequisites**: BIO 30 (Biochemistry) - Spring 2026  
**Mission**: Advocate for kidney disease patients and transplant recipients

**å‰ç½®è¦æ±‚**ï¼šBIO 30ï¼ˆç”Ÿç‰©åŒ–å­¸ï¼‰- 2026å¹´æ˜¥å¤©  
**ä½¿å‘½**ï¼šç‚ºè…ç—…æ‚£è€…å’Œç§»æ¤å—è´ˆè€…ç™¼è²

**Research Focus**:
- **Apply AI/ML to kidney exosome stem cell therapy research**
- **Develop predictive models for transplant rejection using biochemical markers**
- **Collaborate with regenerative medicine labs on data analysis**
- **Bridge computer science and biomedical research for patient advocacy**

**ç ”ç©¶é‡é»**ï¼š
- **æ‡‰ç”¨AI/MLæ–¼è…è‡Ÿå¤–æ³Œé«”å¹¹ç´°èƒæ²»ç™‚ç ”ç©¶**
- **é–‹ç™¼ä½¿ç”¨ç”ŸåŒ–æ¨™è¨˜é æ¸¬ç§»æ¤æ’æ–¥çš„é æ¸¬æ¨¡å‹**
- **èˆ‡å†ç”Ÿé†«å­¸å¯¦é©—å®¤åˆä½œé€²è¡Œæ•¸æ“šåˆ†æ**
- **æ©‹æ¥è¨ˆç®—æ©Ÿç§‘å­¸èˆ‡ç”Ÿç‰©é†«å­¸ç ”ç©¶ï¼Œç‚ºç—…æ‚£æ¬Šç›Šç™¼è²**

**Personal Motivation**: As a kidney transplant recipient, this research combines my CS skills with my lived experience to advance treatments and give voice to those suffering from kidney disease.

**å€‹äººå‹•æ©Ÿ**ï¼šä½œç‚ºè…ç§»æ¤å—è´ˆè€…ï¼Œé€™é …ç ”ç©¶å°‡æˆ‘çš„CSæŠ€èƒ½èˆ‡è¦ªèº«ç¶“æ­·ç›¸çµåˆï¼Œä»¥æ¨é€²æ²»ç™‚ä¸¦ç‚ºè…ç—…æ‚£è€…ç™¼è²ã€‚

**Expected Outcomes**:
- Novel AI algorithms for predicting transplant outcomes
- Open-source tools for regenerative medicine researchers
- Patient advocacy through data-driven insights
- Publications bridging CS and biomedical fields

**é æœŸæˆæœ**ï¼š
- é æ¸¬ç§»æ¤çµæœçš„æ–°ç©AIæ¼”ç®—æ³•
- ç‚ºå†ç”Ÿé†«å­¸ç ”ç©¶äººå“¡æä¾›çš„é–‹æºå·¥å…·
- é€éæ•¸æ“šé©…å‹•çš„æ´å¯Ÿé€²è¡Œæ‚£è€…å€¡å°
- æ©‹æ¥CSèˆ‡ç”Ÿç‰©é†«å­¸é ˜åŸŸçš„å‡ºç‰ˆç‰©

---

#### 3. **Extended Reality (XR) Integration**

- VR version of Hunters Point map using Unity/Unreal Engine
- AR volunteer training scenarios with real-world overlay
- Measure conversion rate increase with immersive technology
- Collaborate with Stanford XR Lab (potential research partnership)

---

#### 4. **Multiplayer Competitive Mode**

- Head-to-head algorithm races in real-time
- Global leaderboards by Elo rating with ranking tiers
- Team-based collaborative challenges for CS courses
- Integration with educational platforms (Canvas LMS)

---

#### 5. **Advanced Data Analytics Pipeline**

- Automated user study data collection (n=100+ players)
- Statistical significance testing for learning improvements
- Longitudinal study tracking algorithm retention over 6 months
- Publication-ready analysis for AI education conferences (AAAI, EDM)

---

## ğŸ“Š Appendix A: Complexity Analysis Summary | è¤‡é›œåº¦åˆ†æç¸½çµ

| Algorithm | Time Complexity | Space Complexity | Optimal? | Use Case |
|-----------|----------------|------------------|----------|----------|
| A* Search | O(b^d) | O(b^d) | Yes (with admissible heuristic) | Pathfinding |
| Alpha-Beta | O(b^(m/2)) | O(bm) | Yes (with optimal ordering) | Decision trees |
| Bayesian Net | O(nÂ²) | O(nÂ²) | Yes | Probabilistic inference |
| Tower of Hanoi | O(2^n) | O(n) | Yes | Recursive planning |
| N-Queens | O(n!) | O(nÂ²) | Yes (with backtracking) | Constraint satisfaction |
| Hill Climbing | O(n) | O(1) | No (local optima) | Optimization |
| First-Order Logic | O(2^n) | O(n) | Yes | Logical inference |
| Blocks World | O(n!) | O(nÂ²) | Yes (with STRIPS) | Planning |

---

## ğŸ“š References | åƒè€ƒæ–‡ç»

Russell, S., & Norvig, P. (2021). *Artificial Intelligence: A Modern Approach* (4th ed.). Pearson.

Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. *Journal of Artificial Intelligence Research*, 11, 1-94.

---

## ğŸ“§ Contact for Technical Questions | æŠ€è¡“å•é¡Œè¯çµ¡

**Student**: è¨±ç¾å«» (Mei Hsien Hsu)  
**Email**: hsu.meihsien@gmail.com  
**GitHub**: [@AAdl11](https://github.com/AAdl11)  
**Repository**: https://github.com/AAdl11/meihsien  
**Live Demo**: https://aadl11.github.io/meihsien/

---

## ğŸ™ Acknowledgments | è‡´è¬

**Special Thanks To | ç‰¹åˆ¥æ„Ÿè¬**:

- **Professor An Lam** (Las Positas College) - CS4 instruction and mentorship  
- **Russell & Norvig** - Foundational textbook: *Artificial Intelligence: A Modern Approach* (2021, 4th ed.)  
- **Sister Roxanne** (My Mentor) - Witnessed the Raw Rice Incident (2000), made the life-changing decision to leave Genentech and dedicate 25 years to serving the Bayview-Hunters Point community, and taught me the spirit and philosophy of volunteer service with Tzu Chi Foundation  
- **Tzu Chi Foundation San Francisco** - Platform for 25 years of community service and volunteer coordination  
- **Bayview-Hunters Point Community** - Real-world scenarios, ongoing collaboration, and 8,000+ families served  
- **CS4 Classmates** - Beta testing and valuable feedback  
- **Dharma Master Cheng Yen** - Philosophical foundation of "Great Love, Great Compassion"
- **Kidney Disease Community** - Inspiration to pursue biomedical AI research for patient advocacy

---

**Academic Pathway | å­¸è¡“è·¯å¾‘**:
- **Current**: CS4 (AI Introduction) - Fall 2025
- **Spring 2026**: CS5 (Machine Learning) + BIO 30 (Biochemistry)
- **Goal**: AI Certificate + Foundation for kidney exosome stem cell research
- **Mission**: Advocate for kidney disease patients through AI/ML research

---

**Document Version**: 2.0  
**Last Updated**: November 3, 2025  
**Status**: Phase 2 Complete - Ready for Professor Review

---

**Â© 2025 è¨±ç¾å«» (Mei Hsien Hsu) | Journey of Kindness | CS4 Final Project | Las Positas College**

**Honoring Sister Roxanne's 25 years of transformative service (2000-2025)**  
**Advocating for kidney disease patients and transplant recipients through biomedical AI research ğŸ’šğŸ§¬**